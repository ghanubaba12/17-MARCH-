{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e97a00-4add-436c-8b36-63f4fab7af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "ans-\n",
    "Missing values in a dataset are entries that have no data or information recorded for a particular variable or feature. They are represented by special symbols or codes such as \"NA,\" \"NaN,\" or \"null\" in most programming languages. Missing values can be due to a variety of reasons, such as data entry errors, equipment malfunction, or survey non-response.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can affect the accuracy and validity of statistical analyses and machine learning algorithms. If missing values are not handled correctly, they can introduce bias, reduce statistical power, and lead to incorrect conclusions. Therefore, data analysts and data scientists need to address missing values in their data cleaning and preprocessing steps before performing any analysis or modeling.\n",
    "\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and XGBoost. These algorithms can handle missing values by either ignoring them or treating them as a separate category. Other algorithms, such as linear regression and logistic regression, require imputation or deletion of missing values before they can be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252b148-4f7f-4059-bf98-09f16c47cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "ans-\n",
    "There are several techniques used to handle missing data, including:\n",
    "\n",
    "Deletion: In this technique, rows or columns with missing data are removed from the dataset. There are two types of deletion methods:\n",
    "a. Listwise deletion: It involves removing all the records that contain missing values.\n",
    "b. Pairwise deletion: It involves removing only the records that have missing values for a specific variable.\n",
    "Example of Listwise deletion using pandas library in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [np.nan, 10, 11, 12]})\n",
    "\n",
    "# remove all rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n",
    "Output:\n",
    "\n",
    "r\n",
    "Copy code\n",
    "   A    B    C\n",
    "0  1.0  5.0  NaN\n",
    "1  2.0  NaN  10.0\n",
    "2  NaN  7.0  11.0\n",
    "3  4.0  8.0  12.0\n",
    "Imputation: In this technique, missing data is replaced with estimated or predicted values. There are several imputation methods, including mean imputation, median imputation, mode imputation, and K-nearest neighbor imputation.\n",
    "Example of mean imputation using scikit-learn library in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [np.nan, 10, 11, 12]})\n",
    "\n",
    "# create an imputer object and specify mean imputation strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit the imputer on the dataset\n",
    "imputer.fit(df)\n",
    "\n",
    "# transform the dataset with imputed values\n",
    "df_imputed = pd.DataFrame(imputer.transform(df), columns=df.columns)\n",
    "\n",
    "print(df_imputed)\n",
    "Output:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "     A     B     C\n",
    "0  1.0  5.00  10.5\n",
    "1  2.0  6.67  10.0\n",
    "2  2.33 7.00  11.0\n",
    "3  4.0  8.00  12.0\n",
    "Model-based imputation: In this technique, missing data is imputed using a predictive model trained on the non-missing values. Some examples of model-based imputation techniques are regression imputation, Bayesian imputation, and decision tree imputation.\n",
    "Example of regression imputation using statsmodels library in Python:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [np.nan, 10, 11, 12]})\n",
    "\n",
    "# create a regression model using non-missing values\n",
    "model = sm.OLS(df['B'].dropna(), sm.add_constant(df[['A', 'C']].dropna()))\n",
    "\n",
    "# predict the missing values using the regression model\n",
    "df['B'] = model.predict(sm.add_constant(df[['A', 'C']])).values\n",
    "\n",
    "print(df)\n",
    "Output:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "     A     B     C\n",
    "0  1.0  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc08068-d4cc-4530-89c9-717edfa2dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "ans-\n",
    "Imbalanced data refers to a situation where the distribution of classes in a classification problem is not equal, and one or more classes are significantly underrepresented compared to the others. For example, in a binary classification problem, if the positive class has only 5% of the data while the negative class has 95% of the data, then the dataset is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can have several negative impacts on the model's performance, including:\n",
    "\n",
    "Bias in model predictions: If the model is trained on imbalanced data, it will be biased towards the majority class, leading to poor performance in predicting the minority class.\n",
    "\n",
    "Poor model generalization: If the model is not exposed to enough samples of the minority class, it may not generalize well to new data in the real world.\n",
    "\n",
    "Misleading evaluation metrics: Evaluation metrics such as accuracy, precision, and recall may not be suitable for imbalanced datasets. For example, a model that always predicts the majority class will have high accuracy but poor performance in predicting the minority class.\n",
    "\n",
    "To overcome these issues, several techniques can be used to handle imbalanced data, such as:\n",
    "\n",
    "Undersampling: It involves randomly removing samples from the majority class to balance the distribution.\n",
    "\n",
    "Oversampling: It involves creating synthetic samples of the minority class to balance the distribution.\n",
    "\n",
    "Hybrid methods: It involves a combination of undersampling and oversampling techniques.\n",
    "\n",
    "Cost-sensitive learning: It involves assigning different misclassification costs to different classes during model training.\n",
    "\n",
    "Ensemble methods: It involves using multiple models to balance the distribution, such as bagging, boosting, or stacking.\n",
    "\n",
    "By handling imbalanced data, we can improve the model's performance in predicting the minority class and avoid the negative impacts of imbalanced data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a572c44-5c5f-4a7d-b23c-a3aeadaeabb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "ans-\n",
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced datasets by balancing the class distribution.\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class to balance the distribution. This can be achieved by randomly duplicating existing samples from the minority class or by generating new synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Here is an example of up-sampling using the Python library scikit-learn:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.utils import resample\n",
    "X_upsampled, y_upsampled = resample(X_minority, y_minority, n_samples=len(X_majority), replace=True, random_state=42)\n",
    "Down-sampling involves reducing the number of samples in the majority class to balance the distribution. This can be achieved by randomly removing samples from the majority class until the class distribution is balanced.\n",
    "\n",
    "Here is an example of down-sampling using the Python library scikit-learn:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.utils import resample\n",
    "X_downsampled, y_downsampled = resample(X_majority, y_majority, n_samples=len(X_minority), replace=False, random_state=42)\n",
    "Up-sampling is generally preferred when the dataset is small, and the minority class has a small number of samples, while down-sampling is preferred when the dataset is large, and the majority class has a large number of samples.\n",
    "\n",
    "For example, suppose we have a dataset of credit card transactions, where only 1% of the transactions are fraudulent. In this case, the dataset is highly imbalanced, and we need to balance the class distribution before training a machine learning model to detect fraud. Up-sampling can be used in this case to generate new synthetic samples of the minority class, while down-sampling can be used if the majority class has too many samples and the dataset size is too large.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f170d-2f26-4ad5-81e9-15d583ab04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "ans-\n",
    "Data augmentation is a technique used to increase the size of a dataset by creating new synthetic samples from the existing ones. This technique is commonly used in machine learning to overcome the problem of overfitting and improve the model's generalization.\n",
    "\n",
    "There are several techniques for data augmentation, such as rotation, scaling, translation, flipping, and adding noise to the data. By applying these transformations to the original data, new samples are generated, increasing the size of the dataset.\n",
    "\n",
    "One popular data augmentation technique for handling imbalanced datasets is Synthetic Minority Over-sampling Technique (SMOTE). SMOTE works by creating synthetic samples of the minority class by interpolating between the existing samples.\n",
    "\n",
    "Here is how SMOTE works:\n",
    "\n",
    "For each sample in the minority class, k-nearest neighbors are found from the minority class.\n",
    "\n",
    "A new synthetic sample is generated by interpolating between the original sample and one of its k-nearest neighbors, chosen randomly.\n",
    "\n",
    "The process is repeated until the desired number of synthetic samples is generated.\n",
    "\n",
    "Here is an example of SMOTE using the Python library imbalanced-learn:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "SMOTE is a popular data augmentation technique because it is simple, effective, and easy to implement. It can improve the performance of machine learning models on imbalanced datasets by creating new synthetic samples of the minority class, which increases the model's exposure to the minority class and improves its ability to generalize to new data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ca2a5-5357-40fa-8a20-9f3d09364341",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "ans-Outliers are data points that are significantly different from the other data points in the dataset. They can occur due to various reasons such as measurement errors, data entry errors, or due to the natural variability of the data.\n",
    "\n",
    "Handling outliers is important because they can significantly affect the statistical analysis of the dataset and the performance of machine learning models. Outliers can distort the distribution of the data, affecting the mean and standard deviation, which can impact the performance of algorithms that rely on these measures.\n",
    "\n",
    "Outliers can also lead to overfitting, where the model learns to fit the outliers, which can lead to poor generalization performance on new data. Therefore, it is important to identify and handle outliers in a dataset.\n",
    "\n",
    "There are several techniques for handling outliers, such as:\n",
    "\n",
    "Removal of outliers: This involves removing the data points that are identified as outliers. However, this approach can be risky as it can lead to the loss of important information and affect the overall analysis of the dataset.\n",
    "\n",
    "Transformation: Transforming the data can help handle outliers. For example, taking the logarithm or square root of the data can help reduce the impact of outliers.\n",
    "\n",
    "Imputation: Replacing the outliers with a suitable value can help handle them. For example, replacing the outliers with the median or mean value of the data can help handle them.\n",
    "\n",
    "Robust models: Robust models are less sensitive to outliers and can handle them better. For example, decision trees and random forests are robust models that can handle outliers.\n",
    "\n",
    "In conclusion, handling outliers is important because they can significantly affect the analysis of the dataset and the performance of machine learning models. Therefore, it is important to identify and handle outliers appropriately.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Outliers are data points that are significantly different from the other data points in the dataset. They can occur due to various reasons such as measurement errors, data entry errors, or due to the natural variability of the data.\n",
    "\n",
    "Handling outliers is important because they can significantly affect the statistical analysis of the dataset and the performance of machine learning models. Outliers can distort the distribution of the data, affecting the mean and standard deviation, which can impact the performance of algorithms that rely on these measures.\n",
    "\n",
    "Outliers can also lead to overfitting, where the model learns to fit the outliers, which can lead to poor generalization performance on new data. Therefore, it is important to identify and handle outliers in a dataset.\n",
    "\n",
    "There are several techniques for handling outliers, such as:\n",
    "\n",
    "Removal of outliers: This involves removing the data points that are identified as outliers. However, this approach can be risky as it can lead to the loss of important information and affect the overall analysis of the dataset.\n",
    "\n",
    "Transformation: Transforming the data can help handle outliers. For example, taking the logarithm or square root of the data can help reduce the impact of outliers.\n",
    "\n",
    "Imputation: Replacing the outliers with a suitable value can help handle them. For example, replacing the outliers with the median or mean value of the data can help handle them.\n",
    "\n",
    "Robust models: Robust models are less sensitive to outliers and can handle them better. For example, decision trees and random forests are robust models that can handle outliers.\n",
    "\n",
    "In conclusion, handling outliers is important because they can significantly affect the analysis of the dataset and the performance of machine learning models. Therefore, it is important to identify and handle outliers appropriately.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f6ee6-9dda-41fa-99be-d625a96f9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "ans-\n",
    "Missing data can be a significant challenge in data analysis as it can lead to biased results and reduce the accuracy of your analysis. Here are some techniques to handle missing data in your analysis:\n",
    "\n",
    "Delete the missing data: If the percentage of missing data is small, one option is to delete the records containing missing values. This approach is also called complete case analysis, and it is the simplest method to handle missing data. However, if the missing data is extensive, deleting records can significantly reduce the sample size and affect the accuracy of the analysis.\n",
    "\n",
    "Impute missing data: Imputation is the process of estimating missing values based on the values of other variables in the dataset. There are several techniques for imputing missing data, including mean imputation, median imputation, regression imputation, and multiple imputation. Imputation techniques can help to minimize bias in the analysis caused by missing data.\n",
    "\n",
    "Use machine learning algorithms: Machine learning algorithms, such as decision trees and random forests, can handle missing data in a robust way by using a splitting criterion that can handle missing values. These algorithms can also impute missing data and make predictions based on the imputed values.\n",
    "\n",
    "Use sensitivity analysis: Sensitivity analysis involves testing the robustness of the results by repeating the analysis with different assumptions about the missing data. This approach can help to identify the impact of missing data on the analysis results and assess the reliability of the findings.\n",
    "\n",
    "In conclusion, handling missing data in your analysis requires careful consideration of the extent of the missing data, the nature of the variables, and the goals of the analysis. Choosing the appropriate technique for handling missing data can improve the accuracy and reliability of your analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d4ce4-c9ba-4862-b0fb-496636c0a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "ans-There are several strategies you can use to determine if the missing data is missing at random (MAR) or if there is a pattern to the missing data, such as:\n",
    "\n",
    "Visualize missing data: Visualizing the distribution of missing values across different variables in the dataset can provide insights into the pattern of missing data. For example, a heatmap or a dendrogram can show which variables have missing values and how the missing values are related to other variables in the dataset.\n",
    "\n",
    "Conduct statistical tests: Statistical tests can help to determine if the missing data is MAR or not. One common test is the Little's MCAR test, which tests the null hypothesis that the missing data is MCAR. If the test fails to reject the null hypothesis, it suggests that the missing data is MAR.\n",
    "\n",
    "Compare missing and non-missing data: Another approach is to compare the characteristics of the missing and non-missing data to identify any patterns or differences. For example, you can compare the mean, median, or distribution of the variables with and without missing values to see if there are any significant differences.\n",
    "\n",
    "Analyze the mechanism of missingness: Understanding the mechanism of missingness can help to identify the patterns of missing data. There are three mechanisms of missingness: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). MCAR means that the probability of missing data is unrelated to any observed or unobserved variables. MAR means that the probability of missing data is related to observed variables but not to unobserved variables. MNAR means that the probability of missing data is related to both observed and unobserved variables.\n",
    "\n",
    "In conclusion, determining the pattern of missing data is an important step in data analysis as it can affect the validity of the results. Using a combination of visualization, statistical tests, and comparison of missing and non-missing data can help to identify the pattern of missing data and determine the appropriate strategy for handling it.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356a1a0-3adc-417b-9b53-ccd2ff30339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "ans-\n",
    "Dealing with imbalanced datasets can be a challenging task in machine learning. In medical diagnosis projects, the imbalance issue is common where the majority of patients do not have the condition of interest. Here are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset:\n",
    "\n",
    "Resampling Techniques: Resampling techniques include oversampling and undersampling. Oversampling involves increasing the number of instances of the minority class, while undersampling involves decreasing the number of instances of the majority class. These techniques can help to balance the dataset and improve the performance of the model.\n",
    "\n",
    "Evaluation Metrics: Evaluation metrics such as accuracy are not reliable for imbalanced datasets. Therefore, it is essential to use metrics such as precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) that provide a more comprehensive evaluation of the model's performance.\n",
    "\n",
    "Cost-Sensitive Learning: In cost-sensitive learning, the misclassification costs are adjusted to reflect the importance of correctly classifying the minority class. This technique can help to improve the performance of the model on the minority class.\n",
    "\n",
    "Ensemble Learning: Ensemble learning involves combining multiple models to improve the performance of the model. Techniques such as bagging and boosting can help to improve the performance of the model on the minority class.\n",
    "\n",
    "Cross-Validation: Cross-validation is a technique that involves dividing the dataset into training and testing sets. This technique can help to evaluate the performance of the model on multiple subsets of the data and reduce the risk of overfitting.\n",
    "\n",
    "In conclusion, dealing with imbalanced datasets is a challenging task in machine learning. Using resampling techniques, appropriate evaluation metrics, cost-sensitive learning, ensemble learning, and cross-validation can help to improve the performance of the model on the minority class and provide more accurate predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecbd71-bad7-4be0-ab2e-9cbd0940571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "ans-When dealing with an imbalanced dataset in customer satisfaction analysis, one option is to balance the dataset by downsampling the majority class. Here are some methods you can use to balance the dataset and down-sample the majority class:\n",
    "\n",
    "Random Undersampling: Random undersampling involves randomly selecting a subset of the majority class samples to match the size of the minority class. This method is simple to implement but may lead to a loss of information and reduced accuracy.\n",
    "\n",
    "Tomek Links: Tomek Links are pairs of samples from different classes that are close to each other, and removing the majority class samples from these pairs can help to reduce the imbalance. This method preserves the information of the data but may not be effective if the data is highly overlapping.\n",
    "\n",
    "Cluster-Based Undersampling: Cluster-based undersampling involves clustering the majority class samples and selecting a representative sample from each cluster to match the size of the minority class. This method preserves the information of the data and can be effective in reducing the imbalance.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE involves generating synthetic samples for the minority class by creating new instances of the minority class based on the existing samples. This method can help to balance the dataset and improve the accuracy of the model.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN): ADASYN is a variant of SMOTE that generates more synthetic samples for the minority class samples that are harder to learn. This method can help to balance the dataset and improve the accuracy of the model.\n",
    "\n",
    "In conclusion, balancing an unbalanced dataset in customer satisfaction analysis is an essential step in improving the accuracy of the model. Random undersampling, Tomek Links, cluster-based undersampling, SMOTE, and ADASYN are some methods that can be employed to down-sample the majority class and balance the dataset. The choice of the method depends on the characteristics of the data and the research question at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dff644-1c22-45e8-b00d-b3baada6c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "ANS-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f5c90-489a-42e6-a055-f0e262d93383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369d779-3313-46f2-98bd-1a21ff86b280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f76a68-00de-4a4d-93ec-f0db501dab38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9f715-077f-4e73-b852-0543ee0c9a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c211d-3c35-4e21-8e22-e364c1b1d763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
